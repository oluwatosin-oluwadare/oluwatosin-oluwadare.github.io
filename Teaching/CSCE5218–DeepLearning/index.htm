
<!DOCTYPE html>
<!-- saved from url=(0042)https://cse.buffalo.edu/~jmeng2/index.html -->
<html lang="en">

<head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
	<!-- Google tag (gtag.js) -->
	<script async src="https://www.googletagmanager.com/gtag/js?id=G-KD9REKPKLG"></script>
	<script>
	  window.dataLayer = window.dataLayer || [];
	  function gtag(){dataLayer.push(arguments);}
	  gtag('js', new Date());
	
	  gtag('config', 'G-KD9REKPKLG');
	</script>
    
    <title>CSCE 5218 - Deep Learning</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" type="text/css" href="../../stylefiles/global.css">
    <link rel="stylesheet" type="text/css" href="../../stylefiles/navigation.css">
	<link rel="stylesheet" type="text/css" href="../../stylefiles/home.css">
	<link rel="icon" type="image/png" href="Image/icon.png">
	<link rel="stylesheet" href="styles.css"> 
	
</head>

<body data-gr-c-s-loaded="true">

<style type="text/css">
div img{
  cursor: pointer;
  transition: all 0.6s;
}
div img:hover{
  transform: scale(1.8);
}
.underlinedist{ padding-bottom:3px; border-bottom:1px solid #000} 
</style>


<div class="central_body">
	
	
	
    <font size="5"><center><b>CSCE 5218 – Deep Learning</b></center></font>

	
    <p style="margin-top:5px;">
	<center>
	<font size="4"> <b>Spring 2026</b></font> &nbsp; &nbsp; 
	<br>
	</center>
	</p>
	
	<HR>
	
	<p style="margin-top:5px;">
	<font size="4"> <b>Basic information:</b></font> <br>
	</p>
	
	<p style="text-align:justify">
	<ul>
		<li>
			<font size="4"> Instructor: <a href="https://oluwadarelab.com/oluwatosin-oluwadare/" target="_blank"><font color="#0099ff">Oluwatosin Oluwadare</font></a> (Oluwatosin.Oluwadare@unt.edu)</font>
		</li>
		<li>
			<font size="4"> Office: Discovery Park F209 </font>
		</li>
		<li>
			<font size="4"> Office hours: Thurs 10:00 am-12:00 pm or by appointment</font>
		</li>
		<br>
		<li>
			<font size="4"> Lecture time: Tues 1:00 - 2:20 pm </font>
		</li>
		<!--<li>
			<font size="4"> Classroom: NTDP K110</font>
		</li>
		-->
		<li>
			<font size="4"> Syllabus: <a href=".\Deep Learning-Syllabus-Spring 2026.pdf" target="_blank"><font color="#0099ff">PDF</font></a></font>
		</li>
		
		<br>
		<li>
			<font size="4"> TA: Minghao Li (minghaoli@my.unt.edu)</font>
			<br>
			<font size="4">Office Hours: <!--3:00-5:00 pm on Wed, F221 --> or by appointment</font>
		</li>
		<br>
		<!--
		<li>
			<font size="4"> IA: Piyush Deepak Hemnani (PiyushHemnani@my.unt.edu)</font>
			<br>
			<font size="4"> Office Hours: 12:00-2:00 pm on Friday (via appointment)</font>
		</li>
		-->
	</ul>
	</p>
	
	<HR>
	
	<p style="margin-top:5px;">
	<font size="4"> <b>Course description</b></font>
	</p>
	
	<p style="text-align:justify">
	<font size="4">This course aims at covering the basics of modern deep neural networks. In specific, the first part will introduce the fundamental concepts in neural networks including network architecture, activation function, loss, optimization, gradient and initializations etc. Then, the second part will describe specific types of different deep neural networks such as convolutional neural networks (CNNs), recurrent neural networks (RNNs) and attention-based Transformer, as well as their applications in computer vision and natural language processing. In the final part we will briefly discuss some recent advanced topics in deep learning including graph neural networks, unsupervised representation learning, deep reinforcement learning, generative adversarial networks (GANs), etc. In this course, the hands-on practice of implementing deep learning algorithms (in Python) will be provided via homeworks and course project.</font>
	</p>
	
	<HR>
	
	<p style="margin-top:5px;">
	<font size="4"> <b>Textbooks</b></font> 
	</p>
	
	<p style="text-align:justify">
	<font size="4">We will have required readings from the following textbook:</font>
	<ul>
	
		<li>
			<font size="4"> <i> Understanding Deep Learning</i>,  by Simon J.D. Prince, 2025.  <a href="https://udlbook.github.io/udlbook/" target="_blank"><font color="#0099ff">online version</font></a></font>
		</li>

	</ul>
	
	<font size="4">Besides, the following textbooks are useful as additional references:</font>
	<ul>
		<li>
			<font size="4"> <i>Deep Learning</i>, by Ian Goodfellow, Yoshua Bengio, and Aaron Courville, 2016. <a href="https://www.deeplearningbook.org/" target="_blank"><font color="#0099ff">online version</font></a></font>
		</li>
		
		<li>
			<font size="4"> <i>Dive into Deep Learning</i>, by Aston Zhang, Zack C. Lipton, Mu Li, and Alex J. Smola, 2019. <a href="https://d2l.ai/index.html" target="_blank"><font color="#0099ff">online version</font></a> <br>(A lot of examples are provided to practice deep learning.)</font>
		</li>
		<li>
			<font size="4"> <i>Neural Networks and Deep Learning</i>, by Michael Nielsen, 2019. <a href="http://neuralnetworksanddeeplearning.com/" target="_blank"><font color="#0099ff">online version</font></a></font>
		</li>
		<li>
			<font size="4"> <i>Introduction to Deep Learning</i>, by Eugene Charniak, 2019. <a href="https://mitpress.mit.edu/books/introduction-deep-learning" target="_blank"><font color="#0099ff">link</font></a></font>
		</li>

	</ul>
	<font size="4">In addition to the textbooks, extra reading materials will be provided as we cover topics. Check out the course website regularly for updated reading materials.</font>
	</p>
	
	<HR>
	
	<p style="margin-top:5px;">
	<font size="4"> <b><font color="#FF0000">Announcements</font></b></font> 
	</p>
	
	<ul>
		<!-- 
		<li>
			<font size="4"> <b>04/13/2023:</b> <a href="./presentation_schedule.htm" target="_blank"><font color="#0099ff">Project presentation schedule</font></a> is out.</font>
		</li>
		<li>
			<font size="4"> <b>04/10/2023:</b> Paper review list 7 is out, due on 4/18.</font>
		</li>
		<li>
			<font size="4"> <b>03/27/2023:</b> Paper review list 6 is out, due on 4/4.</font>
		</li>
		<li>
			<font size="4"> <b>03/15/2023:</b> Paper review list 5 is out, due on 3/28.</font>
		</li>
		<li>
			<font size="4"> <b>03/06/2023:</b> Paper review list 4 is out, due on 3/14.</font>
		</li>
		<li>
			<font size="4"> <b>02/28/2023:</b> Paper review list 3 is out, due on 3/7.</font>
		</li>
		<li>
			<font size="4"> <b>02/22/2023:</b> The project report is due on 4/20.</font>
		</li>
		
		<li>
			<font size="4"> <b>02/22/2023:</b> Project list is out. Check it <font size="4"> <a href="./course_project.htm" target="_blank"><font color="#0099ff">here</font></a></font></font>
		</li>
		<li>
			<font size="4"> <b>02/09/2023:</b> Paper review list 2 is out, due on 2/26.</font>
		</li>
		<li>
			<font size="4"> <b>02/09/2023:</b> Paper review list 1 is out, due on 2/19.</font>
		</li> 
		<li>
			<font size="4"> <b>03/26/2023:</b> The schedule of project presention is <a href="./presentation_schedule.htm" target="_blank"><font color="#0099ff">out</font></a>.</font>
		</li>
		<li>
			<font size="4"> <b>04/08/2025:</b> <a href="./presentation_schedule.htm" target="_blank"><font color="#0099ff">Project presentation schedule</font></a> is out.</font>
		</li>
		<li>
			<font size="4"> <b>04/08/2025:</b> The project report is due on 4/21.</font>
		</li>
		<li>
			<font size="4"> <b>03/03/2025:</b> The project proposal is due on 3/13.</font>
		</li> 
		-->
		<li>
			<font size="4"> <b>01/01/2026:</b> Course website has been launched.</font>
		</li>
	</ul>

	
	
	<p style="margin-top:5px;">
	<font size="4"> <b><font color="#FF0000">Links</font></b></font> 
	</p>
	
	<ul>
		<!--
		<li>
			<font size="4"> <a href="./presentation_schedule.htm" target="_blank"><font color="#0099ff">Course Project Presentation Schedule</font></a></font>
		</li> 
		-->
		<li>
			<font size="4"> <a href="./course_project.htm" target="_blank"><font color="#0099ff">Course Project</font></a></font>
		</li>
		
	</ul>
	
	
	
	<HR>

	<p style="margin-top:5px;">
	<font size="4"> <b><font color="#FF0000">Paper review list</font></b></font> 
	</p>
	
	
	<font size="4"><b>Important:</b> Read the requirements (<a href="./paper_review.htm" target="_blank"><font color="#0099ff">click here</font></a>) for paper review. Here is a <a href="./FanHeng-01-Review-Henriques.pdf" target="_blank"><font color="#0099ff">review example</font></a> for your reference.</font>
	
	(<font size="4" color="#FF0000">Paper review lists will be gradually added.</font>) 

	<table border="1" cellspacing="0" cellpadding="6" style="border-collapse:collapse; width:100%; font-size:16px; line-height:1.35;">
	  <thead>
		<tr style="background:#f2f2f2;">
		  <th style="width:40px;">#</th>
		  <th style="width:100px;">Due Date</th>
		  <th style="width:220px;">Theme</th>
		  <th> Novel Representative Papers for each theme </th>
		</tr>
	  </thead>
	
	  <tbody>
	
		<!-- Papers 1–3 -->
		<tr>
		  <td><b>1</b></td>
		  <td>Feb 6</td>
		  <td rowspan="3">Foundations of Modern Deep Learning</td>
		  <td>
			A. Krizhevsky, I. Sutskever, and G. E. Hinton, 
			"ImageNet classification with deep convolutional neural networks,"
			in <i>Proc. NeurIPS</i>, 2012.
		  </td>
		</tr>
	
		<tr>
		  <td><b>2</b></td>
		  <td></td>
		  <td>
			A. Paszke <i>et al.</i>, 
			"PyTorch: An imperative style, high-performance deep learning library,"
			in <i>Proc. NeurIPS</i>, 2019.
		  </td>
		</tr>
	
		<tr>
		  <td><b>3</b></td>
		  <td></td>
		  <td>
			K. Simonyan and A. Zisserman, 
			"Very deep convolutional networks for large-scale image recognition,"
			in <i>Proc. ICLR</i>, 2015.
		  </td>
		</tr>
	
	   <script> /***
		<!-- Papers 4–6 -->
		<tr>
		  <td><b>4</b></td>
		  <td>Feb 20</td>
		  <td rowspan="3">Deep Architectures & Early Vision Advances</td>
		  <td>
			K. He, X. Zhang, S. Ren, and J. Sun,
			"Deep residual learning for image recognition,"
			in <i>Proc. CVPR</i>, 2016.
		  </td>
		</tr>
	
		<tr>
		  <td><b>5</b></td>
		  <td></td>
		  <td>
			R. Girshick, J. Donahue, T. Darrell, and J. Malik,
			"Rich feature hierarchies for accurate object detection and semantic segmentation,"
			in <i>Proc. CVPR</i>, 2014.
		  </td>
		</tr>
	
		<tr>
		  <td><b>6</b></td>
		  <td></td>
		  <td>
			R. Girshick,
			"Fast R-CNN,"
			in <i>Proc. ICCV</i>, 2015.
		  </td>
		</tr>
	
		<!-- Papers 7–9 -->
		<tr>
		  <td><b>7</b></td>
		  <td>Mar 6</td>
		  <td rowspan="3">Object Detection & Dense Prediction</td>
		  <td>
			S. Ren, K. He, R. Girshick, and J. Sun,
			"Faster R-CNN: Towards real-time object detection with region proposal networks,"
			in <i>Proc. NeurIPS</i>, 2015.
		  </td>
		</tr>
	
		<tr>
		  <td><b>8</b></td>
		  <td></td>
		  <td>
			J. Redmon, S. Divvala, R. Girshick, and A. Farhadi,
			"You only look once: Unified, real-time object detection,"
			in <i>Proc. CVPR</i>, 2016.
		  </td>
		</tr>
	
		<tr>
		  <td><b>9</b></td>
		  <td></td>
		  <td>
			J. Long, E. Shelhamer, and T. Darrell,
			"Fully convolutional networks for semantic segmentation,"
			in <i>Proc. CVPR</i>, 2015.
		  </td>
		</tr>
	
		<!-- Papers 10–12 -->
		<tr>
		  <td><b>10</b></td>
		  <td>Mar 20</td>
		  <td rowspan="3">Sequence Models & Attention Origins</td>
		  <td>
			J. Chung, C. Gulcehre, K. Cho, and Y. Bengio,
			"Empirical evaluation of gated recurrent neural networks on sequence modeling,"
			in <i>Proc. NeurIPS Workshop</i>, 2014.
		  </td>
		</tr>
	
		<tr>
		  <td><b>11</b></td>
		  <td></td>
		  <td>
			J. Donahue <i>et al.</i>,
			"Long-term recurrent convolutional networks for visual recognition and description,"
			in <i>Proc. CVPR</i>, 2015.
		  </td>
		</tr>
	
		<tr>
		  <td><b>12</b></td>
		  <td></td>
		  <td>
			D. Bahdanau, K. Cho, and Y. Bengio,
			"Neural machine translation by jointly learning to align and translate,"
			in <i>Proc. ICLR</i>, 2015.
		  </td>
		</tr>
	
		<!-- Papers 13–15 -->
		<tr>
		  <td><b>13</b></td>
		  <td>Apr. 3</td>
		  <td rowspan="3">Transformers & Vision Transformers</td>
		  <td>
			A. Vaswani <i>et al.</i>,
			"Attention is all you need,"
			in <i>Proc. NeurIPS</i>, 2017.
		  </td>
		</tr>
	
		<tr>
		  <td><b>14</b></td>
		  <td></td>
		  <td>
			A. Dosovitskiy <i>et al.</i>,
			"An image is worth 16×16 words: Transformers for image recognition at scale,"
			in <i>Proc. ICLR</i>, 2021.
		  </td>
		</tr>
	
		<tr>
		  <td><b>15</b></td>
		  <td></td>
		  <td>
			Ł. Kaiser, O. Nachum, A. Roy, and S. Bengio,
			"One model to learn them all,"
			in <i>Proc. ICML</i>, 2017.
		  </td>
		</tr>
	
		<!-- Papers 16–18 -->
		<tr>
		  <td><b>16</b></td>
		  <td>Apr 17</td>
		  <td rowspan="3">Modern Generative Deep Learning</td>
		  <td>
			I. J. Goodfellow <i>et al.</i>, 
			"Generative adversarial nets,"
			in <i>Proc. NeurIPS</i>, 2014.
		  </td>
		</tr>
	
		<tr>
		  <td><b>17</b></td>
		  <td></td>
		  <td>
			J. Ho, A. Jain, and P. Abbeel,
			"Denoising diffusion probabilistic models,"
			in <i>Proc. NeurIPS</i>, 2020.
		  </td>
		</tr>
	
		<tr>
		  <td><b>18</b></td>
		  <td></td>
		  <td>
			P. Dhariwal and A. Nichol,
			"Diffusion models beat GANs on image synthesis,"
			in <i>Proc. NeurIPS</i>, 2021.
		  </td>
		</tr>
	***/ </script>
	  </tbody>
	</table>

	
	
	<HR>


<h1>Deep Learning Course Schedule</h1>
<p class="small-note">
Schedule will be updated as needed by instructor to accomodate student learning.
</p>

<table>

    <colgroup>
        <col style="width:12%;">  <!-- Date -->
        <col style="width:33%;">  <!-- Lecture -->
        <col style="width:35%;">  <!-- Reading -->
        <col style="width:20%;">  <!-- Note -->
    </colgroup>
    <thead>
        <tr>
            <th>Date</th>
            <th>Lecture</th>
            <th>Reading</th>
            <th>Note</th>
        </tr>
    </thead>
    <tbody>
            <!-- Week 1 -->
        <tr>
           <td colspan="4" style="text-align:left;"><b><font size="4" color="#0000FF">Weeks 1–6: Foundations + Basic Neural Networks</b></font></td>
            
        </tr>
    
        <!-- Week 1 -->
        <tr>
            <td>Jan. 13 </td>
            <td><b>Introduction to the course, overview of deep learning</td>
            <td>Chapter 1: Introduction</td>
            <td></td>
        </tr>

        <!-- Week 2 -->
        <tr>
            <td>Jan. 20</td>
            <td><b>Supervised learning</td>
            <td>Chapter 2: Supervised learning</td>
            <td></td>
        </tr>

        <!-- Week 3 -->
        <tr>
            <td>Jan. 27</td>
            <td><b>Shallow neural networks (SNN)</td>
            <td>Chapter 3: Shallow neural networks</td>
            <td></td>
        </tr>

        <!-- Week 4 -->
        <tr>
            <td>Feb. 3</td>
            <td><b> Deep neural networks</td>
            <td>Chapter 4: Deep neural networks</td>
            <td></td>
        </tr>

        <!-- Week 5 -->
        <tr>
            <td>Feb. 10</td>
            <td><b>Loss functions + Optimization: </b> Fitting Models (SGD, Momentum, Adam)</td>
            <td>Chapter 5: Loss functions + Chapter 6: Fitting models</td>
            <td></td>
        </tr>

        <!-- Week 6 -->
        <tr>
            <td>Feb. 17</td>
            <td><b> Training Deep Networks: </b> Gradients, backprop, initialization, optimization, evaluation</td>
            <td>Chapter 7: Gradients and initialization + Chapter 8: Measuring performance</td>
            <td><font size="4" color="#FF0000">Team Assignment Completed (2/20) </font></td>
        </tr>
        
        
		<tr>
			<td colspan="4" style="text-align:left;"><b><font size="4" color="#0000FF">Weeks 7–15: Modern Architectures (CNN → ResNet → RNN → Transformers → GNN → Generative Models)</b></font></td>
		</tr>


        <!-- Week 7 -->
        <tr>
            <td>Feb. 24</td>
            <td><b> Convolutional Neural Networks </b>(CNNs Part 1)</td>
            <td> Chapter 10: Convolutional networks (10.1-10.3)</td>
            <td></td>
        </tr>

        <!-- Week 8 -->
        <tr>
            <td>Mar. 3</td>
            <td><b>CNNs Part 2 </b>(Downsampling, Upsampling, Applications) + Regularization </td>
            <td> Chapter 10: Convolutional networks (10.1-10.3) + Chapter 9: Regularization (9.1 - 9.2)</td>
            <td></td>
        </tr>


        <!-- Week 9 -->
        <tr>
            <td>March 9 - 15, 2026</td>
            <td><b><b> <font size="4" color="#FF0000">Spring break (No Class)</font></td>
            <td></td>
            <td><font size="4" color="#FF0000">Project Proposal Due (3/15)</font></td>
        </tr>

        <!-- Week 10 -->
        <tr>
            <td>Mar. 17</td>
            <td><b>Residual Networks (ResNets)</td>
            <td>Chapter 11: Residual networks</td>
            <td></td>
        </tr>

        
    	 <!-- Week 11 -->
        <tr>
            <td>Mar. 24</td>
            <td><b>Recurrent Neural Networks (RNNs)</b> – sequence modeling, vanishing gradients, LSTM/GRU intro.</td>
            <td>Deep Learning by <a href="https://www.deeplearningbook.org/contents/rnn.html" target="_blank"> <i>Goodfellow et al : Ch 10</i> </a></td>
            <td></td>
        </tr>
        
        <!-- Week 12 -->
        <tr>
            <td>Mar. 31</td>
            <td><b>  Transformers </b> – self-attention, encoder/decoder, applications</td>
            <td>Chapter 12: Transformers</td>
            <td></td>
        </tr>
        
        
		<!-- Week 14 -->
		<tr>
			<td>Apr. 14</td>
			<td><b>Advanced Topic: </b> Graph Neural Networks</td>
			<td>Chapter 13: Graph neural networks</td>
			<td></td>
		</tr>
		
		<!-- Week 15 -->
		<tr>
			<td>Apr. 21</td>
			<td><b>Advanced Topic: </b> GANs or Diffusion Models – forward & reverse process</td>
			<td>Chapter 18: Diffusion models</td>
			<td></td>
		</tr>
		
		<!-- Week 16 -->
		<tr>
			<td>Apr. 28</td>
			<td> (Tentative) Why Deep Learning Works, Ethics + Course Wrap-Up </b></td>
			<td>Chapter 20: Why deep learning works + Chapter 21: Deep learning and ethics</td>
			<td></td>
		</tr>
		
		<!-- Week 17 -->
        <tr>
            <td>May 4 - 8</td>
            <td><b> Finals Week </b></td>
            <td></td>
            <td> <font size="4" color="#FF0000">Project Report Due May 5</font></td>
        </tr>
       
        <!--   
        <tr>
            <td>Apr. 28</td>
            <td>Deep learning and ethics</td>
            <td>Chapter 21: Deep learning and ethics</td>
            <td></td>
        </tr>
        <tr>
            <td></td>
            <td>Why does deep learning work?</td>
            <td>Chapter 20: Why does deep learning work?</td>
            <td></td>
        </tr>
        -->
        
        
    </tbody>
</table>

</body>
</html>





	