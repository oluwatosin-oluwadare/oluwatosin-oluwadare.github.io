
<!DOCTYPE html>
<!-- saved from url=(0042)https://cse.buffalo.edu/~jmeng2/index.html -->
<html lang="en">

<head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <title>CSCE 5218 - Deep Learning</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" type="text/css" href="../../stylefiles/global.css">
    <link rel="stylesheet" type="text/css" href="../../stylefiles/navigation.css">
	<link rel="stylesheet" type="text/css" href="../../stylefiles/home.css">
	<link rel="icon" type="image/png" href="Image/icon.png">
	<style>a{ TEXT-DECORATION:none}a:hover{TEXT-DECORATION:underline }</style>
	<!-- <style>a{ TEXT-DECORATION:none }</style>-->   <!-- change style for hyperlink -->
</head>

<body data-gr-c-s-loaded="true">

<style type="text/css">
div img{
  cursor: pointer;
  transition: all 0.6s;
}
div img:hover{
  transform: scale(1.8);
}
.underlinedist{ padding-bottom:3px; border-bottom:1px solid #000} 
</style>


<div class="central_body">
	
	
	
    <font size="5"><center><b>CSCE 5218 – Deep Learning</b></center></font>

	
    <p style="margin-top:5px;">
	<center>
	<font size="4"> <b>Spring 2026</b></font> &nbsp; &nbsp; 
	<br>
	</center>
	</p>
	
	<HR>
	
	<p style="margin-top:5px;">
	<font size="4"> <b>Basic information:</b></font> <br>
	</p>
	
	<p style="text-align:justify">
	<ul>
		<li>
			<font size="4"> Instructor: <a href="https://oluwadarelab.com/oluwatosin-oluwadare/" target="_blank"><font color="#0099ff">Oluwatosin Oluwadare</font></a> (Oluwatosin.Oluwadare@unt.edu)</font>
		</li>
		<li>
			<font size="4"> Office: Discovery Park F209 </font>
		</li>
		<li>
			<font size="4"> Office hours: Tues 10:00 am-12:00 pm or by appointment</font>
		</li>
		<br>
		<li>
			<font size="4"> Lecture time: Tu 1:00 - 2:20 pm </font>
		</li>
		<!--<li>
			<font size="4"> Classroom: NTDP K110</font>
		</li>
		-->
		<li>
			<font size="4"> Syllabus: <a href=".\Deep Learning-Syllabus-Spring 2026.pdf" target="_blank"><font color="#0099ff">PDF</font></a></font>
		</li>
		
		<br>
		<li>
			<font size="4"> TA: Minghao Li (minghaoli@my.unt.edu)</font>
			<br>
			<font size="4">Office Hours: <!--3:00-5:00 pm on Wed, F221 --> or by appointment</font>
		</li>
		<br>
		<!--
		<li>
			<font size="4"> IA: Piyush Deepak Hemnani (PiyushHemnani@my.unt.edu)</font>
			<br>
			<font size="4"> Office Hours: 12:00-2:00 pm on Friday (via appointment)</font>
		</li>
		-->
	</ul>
	</p>
	
	<HR>
	
	<p style="margin-top:5px;">
	<font size="4"> <b>Course description</b></font>
	</p>
	
	<p style="text-align:justify">
	<font size="4">This course aims at covering the basics of modern deep neural networks. In specific, the first part will introduce the fundamental concepts in neural networks including network architecture, activation function, loss, optimization, gradient and initializations etc. Then, the second part will describe specific types of different deep neural networks such as convolutional neural networks (CNNs), recurrent neural networks (RNNs) and attention-based Transformer, as well as their applications in computer vision and natural language processing. In the final part we will briefly discuss some recent advanced topics in deep learning including graph neural networks, unsupervised representation learning, deep reinforcement learning, generative adversarial networks (GANs), etc. In this course, the hands-on practice of implementing deep learning algorithms (in Python) will be provided via homeworks and course project.</font>
	</p>
	
	<HR>
	
	<p style="margin-top:5px;">
	<font size="4"> <b>Textbooks</b></font> 
	</p>
	
	<p style="text-align:justify">
	<font size="4">We will have required readings from the following textbook:</font>
	<ul>
	
		<li>
			<font size="4"> <i> Understanding Deep Learning</i>,  by Simon J.D. Prince, 2025.  <a href="https://udlbook.github.io/udlbook/" target="_blank"><font color="#0099ff">online version</font></a></font>
		</li>

	</ul>
	
	<font size="4">Besides, the following textbooks are useful as additional references:</font>
	<ul>
		<li>
			<font size="4"> <i>Deep Learning</i>, by Ian Goodfellow, Yoshua Bengio, and Aaron Courville, 2016. <a href="https://www.deeplearningbook.org/" target="_blank"><font color="#0099ff">online version</font></a></font>
		</li>
		
		<li>
			<font size="4"> <i>Dive into Deep Learning</i>, by Aston Zhang, Zack C. Lipton, Mu Li, and Alex J. Smola, 2019. <a href="https://d2l.ai/index.html" target="_blank"><font color="#0099ff">online version</font></a> <br>(A lot of examples are provided to practice deep learning.)</font>
		</li>
		<li>
			<font size="4"> <i>Neural Networks and Deep Learning</i>, by Michael Nielsen, 2019. <a href="http://neuralnetworksanddeeplearning.com/" target="_blank"><font color="#0099ff">online version</font></a></font>
		</li>
		<li>
			<font size="4"> <i>Introduction to Deep Learning</i>, by Eugene Charniak, 2019. <a href="https://mitpress.mit.edu/books/introduction-deep-learning" target="_blank"><font color="#0099ff">link</font></a></font>
		</li>

	</ul>
	<font size="4">In addition to the textbooks, extra reading materials will be provided as we cover topics. Check out the course website regularly for updated reading materials.</font>
	</p>
	
	<HR>
	
	<p style="margin-top:5px;">
	<font size="4"> <b><font color="#FF0000">Announcements</font></b></font> 
	</p>
	
	<ul>
		<!-- 
		<li>
			<font size="4"> <b>04/13/2023:</b> <a href="./presentation_schedule.htm" target="_blank"><font color="#0099ff">Project presentation schedule</font></a> is out.</font>
		</li>
		<li>
			<font size="4"> <b>04/10/2023:</b> Paper review list 7 is out, due on 4/18.</font>
		</li>
		<li>
			<font size="4"> <b>03/27/2023:</b> Paper review list 6 is out, due on 4/4.</font>
		</li>
		<li>
			<font size="4"> <b>03/15/2023:</b> Paper review list 5 is out, due on 3/28.</font>
		</li>
		<li>
			<font size="4"> <b>03/06/2023:</b> Paper review list 4 is out, due on 3/14.</font>
		</li>
		<li>
			<font size="4"> <b>02/28/2023:</b> Paper review list 3 is out, due on 3/7.</font>
		</li>
		<li>
			<font size="4"> <b>02/22/2023:</b> The project report is due on 4/20.</font>
		</li>
		
		<li>
			<font size="4"> <b>02/22/2023:</b> Project list is out. Check it <font size="4"> <a href="./course_project.htm" target="_blank"><font color="#0099ff">here</font></a></font></font>
		</li>
		<li>
			<font size="4"> <b>02/09/2023:</b> Paper review list 2 is out, due on 2/26.</font>
		</li>
		<li>
			<font size="4"> <b>02/09/2023:</b> Paper review list 1 is out, due on 2/19.</font>
		</li> 
		<li>
			<font size="4"> <b>03/26/2023:</b> The schedule of project presention is <a href="./presentation_schedule.htm" target="_blank"><font color="#0099ff">out</font></a>.</font>
		</li>
		<li>
			<font size="4"> <b>04/08/2025:</b> <a href="./presentation_schedule.htm" target="_blank"><font color="#0099ff">Project presentation schedule</font></a> is out.</font>
		</li>
		<li>
			<font size="4"> <b>04/08/2025:</b> The project report is due on 4/21.</font>
		</li>
		<li>
			<font size="4"> <b>03/03/2025:</b> The project proposal is due on 3/13.</font>
		</li> 
		-->
		<li>
			<font size="4"> <b>01/01/2025:</b> Course website has been launched.</font>
		</li>
	</ul>

	
	
	<p style="margin-top:5px;">
	<font size="4"> <b><font color="#FF0000">Links</font></b></font> 
	</p>
	
	<ul>
		<!--
		<li>
			<font size="4"> <a href="./presentation_schedule.htm" target="_blank"><font color="#0099ff">Course Project Presentation Schedule</font></a></font>
		</li> 
		-->
		<li>
			<font size="4"> <a href="./course_project.htm" target="_blank"><font color="#0099ff">Course Project</font></a></font>
		</li>
		
	</ul>
	
	
	
	<HR>

	<p style="margin-top:5px;">
	<font size="4"> <b><font color="#FF0000">Paper review list</font></b></font> 
	</p>
	
	
	<font size="4"><b>Important:</b> Read the requirements (<a href="./paper_review.htm" target="_blank"><font color="#0099ff">click here</font></a>) for paper review. Here is a <a href="./FanHeng-01-Review-Henriques.pdf" target="_blank"><font color="#0099ff">review example</font></a> for your reference.</font>
	
	(<font size="4" color="#FF0000">Paper review lists will be gradually added.</font>) 

	<p style="text-align:justify">
	<font size="4" ><b>Paper review list 1</b> (<font size="4" color="#FF0000">due on 2/8</font>):</font>
	<ol >
		<li >
			<font size="4" > A Krizhevsky, I Sutskever, and G Hinton, ImageNet Classification with Deep Convolutional Neural Networks, <i>NeurIPS</i>, 2012.
		<li>
			<font size="4" > A Paszke, et al., PyTorch: An Imperative Style, High-Performance Deep Learning Library, <i>NeurIPS</i>, 2019.</font>
		</li>
		
	</ol>
	</p>
	
	
	<p style="text-align:justify">
	<font size="4" ><b>Paper review list 2</b> (<font size="4" color="#FF0000">due on 2/20</font>):</font>
	<ol start="3">
		<li>
			<font size="4" > K Simonyan and A Zisserman, Very Deep Convolutional Networks for Large-Scale Image Recognition, <i>ICLR</i>, 2015.</font>
		</li>
		<li>
			<font size="4" > K. He, X. Zhang, S. Ren, and J. Sun, Deep Residual Learning for Image Recognition, <i>CVPR</i>, 2016.</font>
		</li>
		
	</ol>
	</p>
	
	<p style="text-align:justify">
	<font size="4" ><b>Paper review list 3</b> (<font size="4" color="#FF0000">due on 3/1</font>):</font>
	<ol start="5">
		<li>
			<font size="4" > R. Girshick, J. Donahue, T. Darrell, and J. Malik, Rich feature hierarchies for accurate object detection and semantic segmentation, <i>CVPR</i>, 2014.</font>
		</li>
		<li>
			<font size="4" > R. Girshick, Fast R-CNN, <i>ICCV</i>, 2015.</font>
		</li>
		
	</ol>
	</p>

	<p style="text-align:justify">
		<font size="4" ><b>Paper review list 4</b> (<font size="4" color="#FF0000">due on 3/10</font>):</font>
		<ol start="7">
			<li>
				<font size="4" > S. Ren, K. He, R. Girshick, and J. Sun, Faster R-CNN: Towards real-time object detection with region proposal networks, <i>NIPS</i>, 2015.</font>
			</li>
			<li>
				<font size="4" > J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, You only look once: Unified, real-time object detection, <i>CVPR</i>, 2016.</font>
			</li>
			<li>
				<font size="4" > J. Long, E. Shelhamer, and T. Darrell, Fully Convolutional Networks for Semantic Segmentation, <i>CVPR</i>, 2015.</font>
			</li>
		</ol>
		</p>
	
	
	

    
	<p style="text-align:justify">
	<font size="4" ><b>Paper review list 5</b> (<font size="4" color="#FF0000">due on 3/28</font>):</font>
	<ol start="10">
		<li>
			<font size="4" > J. Chung, C. Gulcehre, K. Cho, and Y. Bengio, Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modelling, <i>NIPS Workshop</i>, 2014.</font>
		</li>
		<li>
			<font size="4" > J. Donahue, L. Hendricks, S. Guadarrama, M. Rohrbach, S. Venugopalan, K. Saenko, and T. Darrell, Long-term recurrent convolutional networks for visual recognition and description, <i>CVPR</i>, 2015.</font>
		</li>
	</ol>
	</p>

	
	<p style="text-align:justify">
	<font size="4" ><b>Paper review list 6</b> (<font size="4" color="#FF0000">due on 4/11</font>):</font>
	<ol start="12">
		<li>
			<font size="4" > D. Bahdanau, K. Cho, and Y. Bengio, Neural machine translation by jointly learning to align and translate, <i>ICLR</i>, 2015.</font>
		</li>
		<li>
			<font size="4" > A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. Gomez, Ł. Kaiser, and I. Polosukhin, Attention is all you need, <i>NIPS</i>, 2017.</font>
		</li>
		<li>
			<font size="4" > A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby, An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale, <i>ICLR</i>, 2021.</font>
		</li>
	</ol>
	</p>
	
	
	<HR>
	<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Deep Learning Class Schedule</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <style>
        body {
            font-family: Arial, Helvetica, sans-serif;
            margin: 0;
            padding: 20px;
            background-color: #f7f7ff;
        }
        h1 {
            text-align: center;
            margin-bottom: 1rem;
        }
        table {
            width: 100%;
            border-collapse: collapse;
            background-color: #ffffff;
        }
        th, td {
            border: 1px solid #ddd;
            padding: 8px 10px;
            vertical-align: top;
        }
        th {
            background-color: #e6e6f5;
            text-align: left;
        }
        tr:nth-child(even) {
            background-color: #fafafa;
        }
        .small-note {
            font-size: 0.9rem;
            color: #555;
            margin-bottom: 10px;
        }
    </style>
</head>
<body>

<h1>Deep Learning Course Schedule</h1>
<p class="small-note">
    Edit the Date and Note columns as needed. Each row corresponds to one week of class.
</p>

<table>
    <thead>
        <tr>
            <th>Date</th>
            <th>Lecture</th>
            <th>Reading</th>
            <th>Note</th>
        </tr>
    </thead>
    <tbody>
        <!-- Week 1 -->
        <tr>
            <td><!-- e.g. Jan 14 --></td>
            <td>Introduction to the course, overview of deep learning</td>
            <td>Chapter 1: Introduction</td>
            <td></td>
        </tr>

        <!-- Week 2 -->
        <tr>
            <td></td>
            <td>Supervised learning</td>
            <td>Chapter 2: Supervised learning</td>
            <td></td>
        </tr>

        <!-- Week 3 -->
        <tr>
            <td></td>
            <td>Shallow neural networks</td>
            <td>Chapter 3: Shallow neural networks</td>
            <td></td>
        </tr>

        <!-- Week 4 -->
        <tr>
            <td></td>
            <td>Deep neural networks</td>
            <td>Chapter 4: Deep neural networks</td>
            <td></td>
        </tr>

        <!-- Week 5 -->
        <tr>
            <td></td>
            <td>Loss functions</td>
            <td>Chapter 5: Loss functions</td>
            <td></td>
        </tr>

        <!-- Week 6 -->
        <tr>
            <td></td>
            <td>Fitting models and optimization</td>
            <td>Chapter 6: Fitting models</td>
            <td></td>
        </tr>

        <!-- Week 7 -->
        <tr>
            <td></td>
            <td>Gradients and initialization</td>
            <td>Chapter 7: Gradients and initialization</td>
            <td></td>
        </tr>

        <!-- Week 8 -->
        <tr>
            <td></td>
            <td>Measuring performance</td>
            <td>Chapter 8: Measuring performance</td>
            <td></td>
        </tr>

        <!-- Week 9 -->
        <tr>
            <td></td>
            <td>Regularization</td>
            <td>Chapter 9: Regularization</td>
            <td></td>
        </tr>

        <!-- Week 10 -->
        <tr>
            <td></td>
            <td>Convolutional networks</td>
            <td>Chapter 10: Convolutional networks</td>
            <td></td>
        </tr>

        <!-- Week 11 -->
        <tr>
            <td></td>
            <td>Residual networks</td>
            <td>Chapter 11: Residual networks</td>
            <td></td>
        </tr>

        <!-- Week 12 -->
        <tr>
            <td></td>
            <td>Transformers</td>
            <td>Chapter 12: Transformers</td>
            <td></td>
        </tr>

        <!-- Week 13 -->
        <tr>
            <td></td>
            <td>Graph neural networks</td>
            <td>Chapter 13: Graph neural networks</td>
            <td></td>
        </tr>

        <!-- Week 14 -->
        <tr>
            <td></td>
            <td>Unsupervised learning</td>
            <td>Chapter 14: Unsupervised learning</td>
            <td></td>
        </tr>

        <!-- Week 15 -->
        <tr>
            <td></td>
            <td>Generative adversarial networks</td>
            <td>Chapter 15: Generative adversarial networks</td>
            <td></td>
        </tr>

        <!-- Week 16 -->
        <tr>
            <td></td>
            <td>Normalizing flows</td>
            <td>Chapter 16: Normalizing flows</td>
            <td></td>
        </tr>

        <!-- Week 17 -->
        <tr>
            <td></td>
            <td>Variational autoencoders</td>
            <td>Chapter 17: Variational autoencoders</td>
            <td></td>
        </tr>

        <!-- Week 18 -->
        <tr>
            <td></td>
            <td>Diffusion models</td>
            <td>Chapter 18: Diffusion models</td>
            <td></td>
        </tr>

        <!-- Week 19 -->
        <tr>
            <td></td>
            <td>Reinforcement learning</td>
            <td>Chapter 19: Reinforcement learning</td>
            <td></td>
        </tr>

        <!-- Week 20 -->
        <tr>
            <td></td>
            <td>Why does deep learning work?</td>
            <td>Chapter 20: Why does deep learning work?</td>
            <td></td>
        </tr>

        <!-- Week 21 -->
        <tr>
            <td></td>
            <td>Deep learning and ethics</td>
            <td>Chapter 21: Deep learning and ethics</td>
            <td></td>
        </tr>
    </tbody>
</table>

</body>
</html>





	